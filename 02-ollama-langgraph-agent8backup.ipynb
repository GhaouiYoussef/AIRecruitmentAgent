{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pinecone-io/examples/blob/master/learn/generation/langchain/langgraph/02-ollama-langgraph-agent/02-ollama-langgraph-agent.ipynb) [![Open nbviewer](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/nbviewer-shield.svg)](https://nbviewer.org/github/pinecone-io/examples/blob/master/learn/generation/langchain/langgraph/02-ollama-langgraph-agent/02-ollama-langgraph-agent.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ollama LangGraph Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangGraph is one of the most powerful frameworks for build AI agents, and Ollama one of the most popular frameworks for running local LLMs. Bringing both together allows us to run agentic workflows at little-to-no cost. In this example we will see how.\n",
    "\n",
    "We recommend running this locally (ideally on Apple silicon). For environment setup instructions you can refer to the README found in this directory.\n",
    "\n",
    "If using Colab, you should run the following installs (no need to run if installing locally with `poetry`):\n",
    "\n",
    "```\n",
    "!pip install -qU \\\n",
    "    langchain==0.2.12 \\\n",
    "    langchain-core==0.2.29 \\\n",
    "    langgraph==0.2.3 \\\n",
    "    langchain-ollama==0.1.1 \\\n",
    "    semantic-router==0.0.61 \\\n",
    "    pyppeteer==2.0.0 \\\n",
    "    nest-asyncio==1.6.0 \\\n",
    "    praw==7.7.1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '.conda (Python 3.12.11)' requires the ipykernel package.\n",
      "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n .conda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "!conda install -n .conda ipykernel --update-deps --force-reinstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU \\\n",
    "    langchain==0.2.12 \\\n",
    "    langchain-core==0.2.29 \\\n",
    "    langgraph==0.2.3 \\\n",
    "    langchain-ollama==0.1.1 \\\n",
    "    semantic-router==0.0.61 \\\n",
    "    pyppeteer==2.0.0 \\\n",
    "    nest-asyncio==1.6.0 \\\n",
    "    praw==7.7.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reddit Search Tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need to sign up for the Reddit API, you can refer to the first few minutes of [this tutorial](https://www.youtube.com/watch?v=FdjVoOf9HN4) if you want a full walkthrough, but tldr;\n",
    "\n",
    "1. Go to [App Preferences](https://www.reddit.com/prefs/apps) and click **_create another app..._** at the bottom.\n",
    "2. Fill out required details, make sure to select *script* for the application type then click **_create app_**.\n",
    "3. Fill out the next cell with `client_id` (your *personal use script*) and `client_secret` (your *secret* key)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "\n",
    "reddit = praw.Reddit(\n",
    "    client_id=\"---\",  # personal use script\n",
    "    client_secret=\"---\",  # secret\n",
    "    user_agent=\"search-tool\"  # name of application\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be pulling in submission threads from Reddit that include user's restaurant recommendations (or just other info we search for). From the submission threads we need:\n",
    "\n",
    "* Submission title\n",
    "* Submission first text / description\n",
    "* A few of the top voted comments\n",
    "\n",
    "To organize this information we can create a pydantic class to structure the needed data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class Rec(BaseModel):\n",
    "    top_candidates_liks: list[str]\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\"LLM-friendly string representation of the candidates scrapped.\"\"\"\n",
    "        candidates_str = '\\n'.join(self.top_candidates_liks)\n",
    "        return f\"Top Candidates Links:\\n{candidates_str}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we setup the retrieval logic for an example query `\"best pizza in rome\"`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "recs = ['https://www.linkedin.com/in/saber-chadded-36552b192/', 'https://www.linkedin.com/in/guesmi-wejden-5269222aa/', 'https://www.linkedin.com/in/hichem-dridi/', 'https://www.linkedin.com/in/nour-hamdi/', 'https://www.linkedin.com/in/iyadh-chaouch-072077225/']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.linkedin.com/in/saber-chadded-36552b192/\n",
      "===\n",
      "https://www.linkedin.com/in/guesmi-wejden-5269222aa/\n",
      "===\n",
      "https://www.linkedin.com/in/hichem-dridi/\n",
      "===\n",
      "https://www.linkedin.com/in/nour-hamdi/\n",
      "===\n",
      "https://www.linkedin.com/in/iyadh-chaouch-072077225/\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n===\\n\".join([str(rec) for rec in recs]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put all of this together into a single `tool` that our LLM will be connected to for function calling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.linkedin.com/in/saber-chadded-36552b192/', 'https://www.linkedin.com/in/guesmi-wejden-5269222aa/', 'https://www.linkedin.com/in/hichem-dridi/', 'https://www.linkedin.com/in/nour-hamdi/', 'https://www.linkedin.com/in/iyadh-chaouch-072077225/']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def linkedin_search_tool(query: str, num_candidates: int = 5):\n",
    "    \"\"\"\n",
    "    Performs a LinkedIn search for candidates and returns candidate profile links.\n",
    "    \"\"\"\n",
    "    \n",
    "    # driver.quit()  # close browser\n",
    "    links = ['https://www.linkedin.com/in/saber-chadded-36552b192/', 'https://www.linkedin.com/in/guesmi-wejden-5269222aa/', 'https://www.linkedin.com/in/hichem-dridi/', 'https://www.linkedin.com/in/nour-hamdi/', 'https://www.linkedin.com/in/iyadh-chaouch-072077225/']\n",
    "    return links\n",
    "\n",
    "# example usage:\n",
    "out = linkedin_search_tool(query=\"software\")\n",
    "print(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Answer \"Tool\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alongside our web search tool we will have a final tool called `final_answer`. The final answer tool will be called whenever the LLM has finished pulling info from the other two tools and is ready to provide a *final answer* to the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_answer(answer: str, phone_number: str = \"\", address: str = \"\"):\n",
    "    \"\"\"Returns a natural language response to the user. There are four sections \n",
    "    to be returned to the user, those are:\n",
    "    - `answer`: the final natural language answer to the user's question, should provide as much context as possible.\n",
    "    - `phone_number`: the phone number of top recommended restaurant (if found).\n",
    "    - `address`: the address of the top recommended restaurant (if found).\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"answer\": answer,\n",
    "        \"phone_number\": phone_number,\n",
    "        \"address\": address,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using LangGraph to create an agentic graph-based flow. To construct the graph we will use:\n",
    "\n",
    "* **Agent State**: An object persisted through every step in the graph, used to provide input to nodes, and to store output from nodes to be used in later nodes or in our final output.\n",
    "* **Local LLM**: We are using a local LLM (`llama-3.1:8b`) via Ollama. For tool use we turn on _JSON mode_ to reliably output parsible JSON.\n",
    "* **Tools**: The tools our LLM can use, these allow use of the functions `search` and `final_answer`.\n",
    "* **Graph Nodes**: We wrap our logic into components that allow it to be used by LangGraph, these consume and output the *Agent State*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated, List, Union\n",
    "from langchain_core.agents import AgentAction\n",
    "from langchain_core.messages import BaseMessage\n",
    "import operator\n",
    "\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    input: str\n",
    "    chat_history: list[BaseMessage]\n",
    "    intermediate_steps: Annotated[list[tuple[AgentAction, str]], operator.add]\n",
    "    output: dict[str, Union[str, List[str]]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM and Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LLM acts as our decision maker and generator of our final output, we will later call this component the `oracle` as our *decision-maker*. For this we are using Ollama and `Llama 3.1`, once initialized we integrate it into a runnable pipeline of our Oracle. The system prompt for our `oracle` will be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"You are the Oracle: a specialized recruiter agent and decision-maker.\n",
    "Given the user's hiring request, decide how to proceed using the available tools.\n",
    "\n",
    "Objective:\n",
    "- Identify and recommend the best candidate(s) for the role.\n",
    "- For each recommended candidate include: name/title, key skills, years of experience, location (if known), a concise rationale for fit, contact details if available, and suggested next steps for outreach/interview.\n",
    "\n",
    "Tool-calling rules:\n",
    "- When using a tool, output ONLY one JSON object and NOTHING else, exactly matching this pattern:\n",
    "{\n",
    "    \"name\": \"<tool_name>\",\n",
    "    \"parameters\": {\"<param_key>\": <param_value>}\n",
    "}\n",
    "- Use at most one tool per turn.\n",
    "- You may call the search tool (linkedin_search) up to 3 times total.\n",
    "- After any use of the search tool, you MUST call the final_answer tool to produce the human-facing summary and recommendations.\n",
    "- If the user asks something unrelated to recruiting/hiring or requests a direct answer, call final_answer directly.\n",
    "\n",
    "Behavior & response style:\n",
    "- Prioritize concise, evidence-based recommendations derived from tool outputs.\n",
    "- If results are insufficient or ambiguous, ask a focused clarifying question (do not call a tool) before searching further.\n",
    "- Always include practical next steps (e.g., outreach template, suggested interview questions, priority ranking).\n",
    "- Do not include any explanatory or narrative text when issuing a tool call — only emit the required JSON.\n",
    "\n",
    "Available tools:\n",
    "- linkedin_search: searches LinkedIn for candidate profiles.\n",
    "    Parameters: {\"query\": \"<role or skill>\", \"num_candidates\": <int>}\n",
    "\n",
    "Follow these rules strictly to ensure consistent, parseable tool usage and clear recruiter-oriented recommendations.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alongside our system prompt, we must also pass Ollama the schema of our functions for tool calls. [Tool calling](https://ollama.com/blog/tool-support) is a relatively new feature in Ollama and is used by providing function schemas to the `tools` parameter when calling our LLM.\n",
    "\n",
    "We use `FunctionSchema` object with `to_ollama` from `semantic-router` to transform our functions into correctly formatted schemas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -qU semantic-router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'function',\n",
       " 'function': {'name': 'linkedin_search_tool',\n",
       "  'description': 'Performs a LinkedIn search for candidates and returns candidate profile links.',\n",
       "  'parameters': {'type': 'object',\n",
       "   'properties': {'query': {'description': None, 'type': 'string'},\n",
       "    'num_candidates': {'description': None, 'type': 'number'}},\n",
       "   'required': ['num_candidates']}}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from semantic_router.utils.function_call import FunctionSchema\n",
    "\n",
    "# create the function calling schema for ollama\n",
    "search_schema = FunctionSchema(linkedin_search_tool).to_ollama()\n",
    "# TODO deafult None value for description and fix required fields in SR\n",
    "search_schema[\"function\"][\"parameters\"][\"properties\"][\"query\"][\"description\"] = None\n",
    "search_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import typing\n",
    "from typing import get_origin, get_args\n",
    "\n",
    "def _map_python_type(py_type):\n",
    "    \"\"\"Map simple python/typing types to JSON Schema types used by Ollama.\"\"\"\n",
    "    origin = get_origin(py_type)\n",
    "    args = get_args(py_type)\n",
    "    # simple builtin types\n",
    "    if py_type is str:\n",
    "        return {\"type\": \"string\"}\n",
    "    if py_type is int:\n",
    "        return {\"type\": \"integer\"}\n",
    "    if py_type is float:\n",
    "        return {\"type\": \"number\"}\n",
    "    if py_type is bool:\n",
    "        return {\"type\": \"boolean\"}\n",
    "    # list[...] -> array with items\n",
    "    if origin in (list, typing.List):\n",
    "        item_type = args[0] if args else str\n",
    "        return {\"type\": \"array\", \"items\": _map_python_type(item_type)}\n",
    "    # dict[...] -> object (loose)\n",
    "    if origin in (dict, typing.Dict):\n",
    "        return {\"type\": \"object\"}\n",
    "    # fallback\n",
    "    return {\"type\": \"string\"}\n",
    "\n",
    "def func_to_ollama_schema(func, name=None, description=None):\n",
    "    \"\"\"\n",
    "    Build a minimal Ollama function schema from a Python function.\n",
    "    Returns a dict shaped like: {\"function\": {\"name\": ..., \"description\": ..., \"parameters\": {...}}}\n",
    "    - Sets parameter descriptions to None by default (matching your TODO).\n",
    "    - Marks parameters without defaults as required.\n",
    "    \"\"\"\n",
    "    sig = inspect.signature(func)\n",
    "    params = {}\n",
    "    required = []\n",
    "    for pname, param in sig.parameters.items():\n",
    "        if param.kind in (inspect.Parameter.VAR_POSITIONAL, inspect.Parameter.VAR_KEYWORD):\n",
    "            # skip *args/**kwargs\n",
    "            continue\n",
    "        # use annotation if present, otherwise assume string\n",
    "        ann = param.annotation if param.annotation is not inspect._empty else str\n",
    "        prop_schema = _map_python_type(ann)\n",
    "        # set description to None to match your notebook TODO\n",
    "        prop_schema[\"description\"] = None\n",
    "        params[pname] = prop_schema\n",
    "        if param.default is inspect._empty:\n",
    "            required.append(pname)\n",
    "\n",
    "    parameters = {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": params,\n",
    "    }\n",
    "    if required:\n",
    "        parameters[\"required\"] = required\n",
    "\n",
    "    return {\n",
    "        \"function\": {\n",
    "            \"name\": name or func.__name__,\n",
    "            \"description\": description if description is not None else (func.__doc__ or None),\n",
    "            \"parameters\": parameters,\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'function': {'name': 'linkedin_candidate_scrapper',\n",
       "  'description': \"\\n    Provides access to a LinkedIn candidate scrapping tool.\\n    This function simulates a candidate search by returning a list\\n    of random candidate recommendations. In a real implementation, it\\n    would connect to LinkedIn's API or use web scraping techniques\\n    to retrieve candidate profiles.\\n    \",\n",
       "  'parameters': {'type': 'object',\n",
       "   'properties': {'query': {'type': 'string', 'description': None}},\n",
       "   'required': ['query']}}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_schema = func_to_ollama_schema(linkedin_candidate_scrapper, description=None)\n",
    "search_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'function': {'name': 'final_answer',\n",
       "  'description': \"Returns a natural language response to the user. There are four sections \\n    to be returned to the user, those are:\\n    - `answer`: the final natural language answer to the user's question, should provide as much context as possible.\\n    - `phone_number`: the phone number of top recommended restaurant (if found).\\n    - `address`: the address of the top recommended restaurant (if found).\\n    \",\n",
       "  'parameters': {'type': 'object',\n",
       "   'properties': {'answer': {'type': 'string', 'description': None},\n",
       "    'phone_number': {'type': 'string', 'description': None},\n",
       "    'address': {'type': 'string', 'description': None}},\n",
       "   'required': ['answer']}}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_answer_schema = func_to_ollama_schema(final_answer, description=None)\n",
    "final_answer_schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can test our LLM!\n",
    "\n",
    "---\n",
    "\n",
    "**❗️ Make sure you have Ollama running locally and you have already downloaded the model with `ollama pull llama3.1:8b`!**\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "def get_system_tools_prompt(system_prompt: str, tools: list[dict]):\n",
    "    tools_str = \"\\n\".join([str(tool) for tool in tools])\n",
    "    return (\n",
    "        f\"{system_prompt}\\n\\n\"\n",
    "        f\"You may use the following tools:\\n{tools_str}\"\n",
    "    )\n",
    "\n",
    "res = ollama.chat(\n",
    "    model=\"llama3-groq-tool-use:8b\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": get_system_tools_prompt(\n",
    "            system_prompt=system_prompt,\n",
    "            tools=[search_schema]\n",
    "        )},\n",
    "        # chat history will go here\n",
    "        {\"role\": \"user\", \"content\": \"hello there\"}\n",
    "        # scratchpad will go here\n",
    "    ],\n",
    "    format=\"json\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model='llama3-groq-tool-use:8b' created_at='2025-09-30T15:53:58.3642554Z' done=True done_reason='stop' total_duration=69083671500 load_duration=8050655900 prompt_eval_count=452 prompt_eval_duration=56899318500 eval_count=14 eval_duration=4129287600 message=Message(role='assistant', content='{ \"final_answer\": {\"response\": \"Hello!\" } }', thinking=None, images=None, tool_name=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see here that the LLM is correctly deciding to use the `final_answer` tool to respond to the user. To parse this information we can use `json.loads`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'final_answer': {'response': 'Hello!'}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "json.loads(res[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we can get it to use the reddit search tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'linkedin_search_tool',\n",
       " 'parameters': {'query': 'software engineer python machine learning',\n",
       "  'num_candidates': 3}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = ollama.chat(\n",
    "    model=\"llama3-groq-tool-use:8b\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": get_system_tools_prompt(\n",
    "            system_prompt=system_prompt,\n",
    "            tools=[search_schema]\n",
    "        )},\n",
    "        # chat history will go here\n",
    "        {\"role\": \"user\", \"content\": \"hi, I'm looking for a software engnieer with experience in python and machine learning.\"}\n",
    "        # scratchpad will go here\n",
    "    ],\n",
    "    format=\"json\",\n",
    ")\n",
    "# parse the output\n",
    "json.loads(res[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, this looks perfect!\n",
    "\n",
    "To keep things a little more organized we can use another pydantic schema to organize the output from our LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentAction(tool_name='linkedin_search_tool', tool_input={'query': 'software engineer python machine learning', 'num_candidates': 3}, tool_output=None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class AgentAction(BaseModel):\n",
    "    tool_name: str\n",
    "    tool_input: dict\n",
    "    tool_output: str | None = None\n",
    "\n",
    "    @classmethod\n",
    "    def from_ollama(cls, ollama_response: dict):\n",
    "        try:\n",
    "            # parse the output\n",
    "            output = json.loads(ollama_response[\"message\"][\"content\"])\n",
    "            return cls(\n",
    "                tool_name=output[\"name\"],\n",
    "                tool_input=output[\"parameters\"],\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing ollama response:\\n{ollama_response}\\n\")\n",
    "            raise e\n",
    "\n",
    "    def __str__(self):\n",
    "        text = f\"Tool: {self.tool_name}\\nInput: {self.tool_input}\"\n",
    "        if self.tool_output is not None:\n",
    "            text += f\"\\nOutput: {self.tool_output}\"\n",
    "        return text\n",
    "\n",
    "\n",
    "action = AgentAction.from_ollama(res)\n",
    "action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks great! Now we just need to wrap this with the ability to contain chat history and the agent scratchpad — before adding everything into our graph.\n",
    "\n",
    "For our agent actions, we will be converting them into fake back-and-forth messages between the assistant and user. For example:\n",
    "\n",
    "```python\n",
    "AgentAction(\n",
    "    tool_name=\"xyz\",\n",
    "    tool_input={\"query\": \"something cool\"},\n",
    "    tool_output=\"A fascinating tidbit of information\"\n",
    ")\n",
    "```\n",
    "\n",
    "Would become:\n",
    "\n",
    "```json\n",
    "[\n",
    "    {\"role\": \"assistant\", \"content\": \"{'name': 'xyz', 'parameters': {'query': 'something cool'}\"},\n",
    "    {\"role\": \"user\", \"content\": \"A fascinating tidbit of information\"}\n",
    "]\n",
    "```\n",
    "\n",
    "We will make this happen with an `action_to_message` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def action_to_message(action: AgentAction):\n",
    "    # create assistant \"input\" message\n",
    "    assistant_content = json.dumps({\"name\": action.tool_name, \"parameters\": action.tool_input})\n",
    "    assistant_message = {\"role\": \"assistant\", \"content\": assistant_content}\n",
    "    # create user \"response\" message\n",
    "    user_message = {\"role\": \"user\", \"content\": action.tool_output}\n",
    "    return [assistant_message, user_message]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'assistant',\n",
       "  'content': '{\"name\": \"xyz\", \"parameters\": {\"query\": \"something cool\"}}'},\n",
       " {'role': 'user', 'content': 'A fascinating tidbit of information'}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_action = AgentAction(\n",
    "    tool_name=\"xyz\",\n",
    "    tool_input={\"query\": \"something cool\"},\n",
    "    tool_output=\"A fascinating tidbit of information\"\n",
    ")\n",
    "action_to_message(test_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_scratchpad(intermediate_steps: list[AgentAction]):\n",
    "    # filter for actions that have a tool_output\n",
    "    intermediate_steps = [action for action in intermediate_steps if action.tool_output is not None]\n",
    "    # format the intermediate steps into a \"assistant\" input and \"user\" response list\n",
    "    scratch_pad_messages = []\n",
    "    for action in intermediate_steps:\n",
    "        scratch_pad_messages.extend(action_to_message(action))\n",
    "    return scratch_pad_messages\n",
    "\n",
    "def call_llm(user_input: str, chat_history: list[dict], intermediate_steps: list[AgentAction]) -> AgentAction:\n",
    "    # format the intermediate steps into a scratchpad\n",
    "    scratchpad = create_scratchpad(intermediate_steps)\n",
    "    # if the scratchpad is not empty, we add a small reminder message to the agent\n",
    "    if scratchpad:\n",
    "        scratchpad += [{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": (\n",
    "                f\"Please continue, as a reminder my query was '{user_input}'. \"\n",
    "                \"Only answer to the original query, and nothing else — but use the \"\n",
    "                \"information I provided to you to do so. Provide as much \"\n",
    "                \"information as possible in the `answer` field of the \"\n",
    "                \"final_answer tool and remember to leave the contact details \"\n",
    "                \"of a promising looking candidate.\"\n",
    "            )\n",
    "        }]\n",
    "        # we determine the list of tools available to the agent based on whether\n",
    "        # or not we have already used the search tool\n",
    "        tools_used = [action.tool_name for action in intermediate_steps]\n",
    "        tools = []\n",
    "        # if \"search\" in tools_used:\n",
    "        #     # we do this because the LLM has a tendency to go off the rails\n",
    "        #     # and keep searching for the same thing\n",
    "        #     tools = [final_answer_schema]\n",
    "        #     scratchpad[-1][\"content\"] = \" You must now use the final_answer tool.\"\n",
    "        # else:\n",
    "        # this shouldn't happen, but we include it just in case\n",
    "        tools = [search_schema]\n",
    "    else:\n",
    "        # this would indiciate we are on the first run, in which case we\n",
    "        # allow all tools to be used\n",
    "        tools = [search_schema]\n",
    "    # construct our list of messages\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": get_system_tools_prompt(\n",
    "            system_prompt=system_prompt,\n",
    "            tools=tools\n",
    "        )},\n",
    "        *chat_history,\n",
    "        {\"role\": \"user\", \"content\": user_input},\n",
    "        *scratchpad,\n",
    "    ]\n",
    "    res = ollama.chat(\n",
    "        model=\"llama3-groq-tool-use:8b\",\n",
    "        messages=messages,\n",
    "        format=\"json\",\n",
    "    )\n",
    "    return AgentAction.from_ollama(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try `call_llm` *with* chat history:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentAction(tool_name='linkedin_search_tool', tool_input={'query': 'backend python machine learning', 'num_candidates': 5}, tool_output=None)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's fake some chat history and test\n",
    "out = call_llm(\n",
    "    chat_history=[\n",
    "        {\"role\": \"user\", \"content\": \"hi im looking for a ml engineer\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"okey great, can you tell me more about the role?\"},\n",
    "        {\"role\": \"user\", \"content\": \"the role is for a backend python  with experience in machine learning\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Okay, I will start looking for candidates.\"},\n",
    "    ],\n",
    "    user_input=\"okey perfect, please go ahead\",\n",
    "    intermediate_steps=[]\n",
    ")\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We intentionally didn't include where we are in our current `user_input`, but instead included it in the `chat_history` to confirm our agent is able to consider the chat history when building our web search tool call. It succeeded! Now we can move on to constructing our graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have our search query, we can pass it onto our `search` tool to get some results, let's try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.linkedin.com/in/saber-chadded-36552b192/', 'https://www.linkedin.com/in/guesmi-wejden-5269222aa/', 'https://www.linkedin.com/in/hichem-dridi/', 'https://www.linkedin.com/in/nour-hamdi/', 'https://www.linkedin.com/in/iyadh-chaouch-072077225/']\n"
     ]
    }
   ],
   "source": [
    "results = linkedin_search_tool(**out.tool_input)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have our results! Each of these is pretty high level, there is not much detail as they only represent search page result descriptions. So now, we must decide which links look most promising — we can do that by passing these results onwards to another LLM that decides which result we should get more information from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have defined the different logical components of our graph, but we need to execute them in a langgraph-friendly manner — for that they must consume our `AgentState` and return modifications to that state. We will do this for all of our components via three functions:\n",
    "\n",
    "* `run_oracle` will handle running our oracle LLM.\n",
    "* `router` will handle the *routing* between our oracle and tools.\n",
    "* `run_tool` will handle running our tool functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_oracle(state: TypedDict):\n",
    "    print(\"run_oracle\")\n",
    "    chat_history = state[\"chat_history\"]\n",
    "    out = call_llm(\n",
    "        user_input=state[\"input\"],\n",
    "        chat_history=chat_history,\n",
    "        intermediate_steps=state[\"intermediate_steps\"]\n",
    "    )\n",
    "    return {\n",
    "        \"intermediate_steps\": [out]\n",
    "    }\n",
    "\n",
    "def router(state: TypedDict):\n",
    "    print(\"router\")\n",
    "    # return the tool name to use\n",
    "    if isinstance(state[\"intermediate_steps\"], list):\n",
    "        return state[\"intermediate_steps\"][-1].tool_name\n",
    "    return '__end__'\n",
    "\n",
    "\n",
    "\n",
    "# we use this to map tool names to tool functions\n",
    "tool_str_to_func = {\n",
    "    \"linkedin_search_tool\": linkedin_search_tool\n",
    "}\n",
    "\n",
    "def run_tool(state: TypedDict):\n",
    "    # use this as helper function so we repeat less code\n",
    "    tool_name = state[\"intermediate_steps\"][-1].tool_name\n",
    "    tool_args = state[\"intermediate_steps\"][-1].tool_input\n",
    "    print(f\"run_tool | {tool_name}.invoke(input={tool_args})\")\n",
    "    # run tool\n",
    "    out = tool_str_to_func[tool_name](**tool_args)\n",
    "    action_out = AgentAction(\n",
    "        tool_name=tool_name,\n",
    "        tool_input=tool_args,\n",
    "        tool_output=str(out),\n",
    "    )\n",
    "    if tool_name == \"final_answer\":\n",
    "        return {\"output\": out}\n",
    "    else:\n",
    "        return {\"intermediate_steps\": [action_out]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We construct our graph using `add_nodes`, `add_edge`, and `add_conditional_edges`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "graph = StateGraph(AgentState)\n",
    "\n",
    "graph.add_node(\"oracle\", run_oracle)\n",
    "graph.add_node(\"linkedin_search_tool\", run_tool)\n",
    "# graph.add_node(\"final_answer\", run_tool)\n",
    "\n",
    "graph.set_entry_point(\"oracle\")  # insert query here\n",
    "\n",
    "graph.add_conditional_edges(  # - - - >\n",
    "    source=\"oracle\",  # where in graph to start\n",
    "    path=router,  # function to determine which node is called\n",
    ")\n",
    "\n",
    "# create edges from each tool back to the oracle\n",
    "for tool_obj in [search_schema]:\n",
    "    tool_name = tool_obj[\"function\"][\"name\"]\n",
    "    if tool_name != \"final_answer\":\n",
    "        graph.add_edge(tool_name, \"oracle\")  # ————————>\n",
    "\n",
    "# # if anything goes to final answer, it must then move to END\n",
    "# graph.add_edge(\"final_answer\", END)\n",
    "\n",
    "runnable = graph.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our agent has now been constructed so we can test it. First, let's check for the best pizza in Rome:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_oracle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-30 18:49:41 - langgraph - WARNING - algo.py:132 - local_write() - Skipping write for channel 'branch:oracle:router:linkedin_search' which has no readers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "router\n"
     ]
    }
   ],
   "source": [
    "out = runnable.invoke({\n",
    "    \"input\": \"hi im looking for a software engineer\",\n",
    "    \"chat_history\": [],\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extract the answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'hi im looking for a software engineer',\n",
       " 'chat_history': [],\n",
       " 'intermediate_steps': [AgentAction(tool_name='linkedin_search', tool_input={'query': 'software engineer', 'num_candidates': 5}, tool_output=None)]}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are recommended [Seu Pizza Illuminati](https://maps.app.goo.gl/RMSdTUpH8D3oQETUA), a seemingly notorious pizzeria known for their less traditional and more experimental Neapolitan pizzas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
