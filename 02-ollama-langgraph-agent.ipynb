{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pinecone-io/examples/blob/master/learn/generation/langchain/langgraph/02-ollama-langgraph-agent/02-ollama-langgraph-agent.ipynb) [![Open nbviewer](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/nbviewer-shield.svg)](https://nbviewer.org/github/pinecone-io/examples/blob/master/learn/generation/langchain/langgraph/02-ollama-langgraph-agent/02-ollama-langgraph-agent.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ollama LangGraph Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangGraph is one of the most powerful frameworks for build AI agents, and Ollama one of the most popular frameworks for running local LLMs. Bringing both together allows us to run agentic workflows at little-to-no cost. In this example we will see how.\n",
    "\n",
    "We recommend running this locally (ideally on Apple silicon). For environment setup instructions you can refer to the README found in this directory.\n",
    "\n",
    "If using Colab, you should run the following installs (no need to run if installing locally with `poetry`):\n",
    "\n",
    "```\n",
    "!pip install -qU \\\n",
    "    langchain==0.2.12 \\\n",
    "    langchain-core==0.2.29 \\\n",
    "    langgraph==0.2.3 \\\n",
    "    langchain-ollama==0.1.1 \\\n",
    "    semantic-router==0.0.61 \\\n",
    "    pyppeteer==2.0.0 \\\n",
    "    nest-asyncio==1.6.0 \\\n",
    "    praw==7.7.1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '.conda (Python 3.12.11)' requires the ipykernel package.\n",
      "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n .conda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "!conda install -n .conda ipykernel --update-deps --force-reinstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU \\\n",
    "    langchain==0.2.12 \\\n",
    "    langchain-core==0.2.29 \\\n",
    "    langgraph==0.2.3 \\\n",
    "    langchain-ollama==0.1.1 \\\n",
    "    semantic-router==0.0.61 \\\n",
    "    pyppeteer==2.0.0 \\\n",
    "    nest-asyncio==1.6.0 \\\n",
    "    praw==7.7.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reddit Search Tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need to sign up for the Reddit API, you can refer to the first few minutes of [this tutorial](https://www.youtube.com/watch?v=FdjVoOf9HN4) if you want a full walkthrough, but tldr;\n",
    "\n",
    "1. Go to [App Preferences](https://www.reddit.com/prefs/apps) and click **_create another app..._** at the bottom.\n",
    "2. Fill out required details, make sure to select *script* for the application type then click **_create app_**.\n",
    "3. Fill out the next cell with `client_id` (your *personal use script*) and `client_secret` (your *secret* key)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "\n",
    "reddit = praw.Reddit(\n",
    "    client_id=\"---\",  # personal use script\n",
    "    client_secret=\"---\",  # secret\n",
    "    user_agent=\"search-tool\"  # name of application\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be pulling in submission threads from Reddit that include user's restaurant recommendations (or just other info we search for). From the submission threads we need:\n",
    "\n",
    "* Submission title\n",
    "* Submission first text / description\n",
    "* A few of the top voted comments\n",
    "\n",
    "To organize this information we can create a pydantic class to structure the needed data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class Rec(BaseModel):\n",
    "    title: str\n",
    "    description: str\n",
    "    comments: list[str]\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\"LLM-friendly string representation of the recommendation(s).\"\"\"\n",
    "        comments_str = '\\n'.join(self.comments)\n",
    "        return f\"Title: {self.title}\\nDescription: {self.description}\\nComments:\\n{comments_str}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we setup the retrieval logic for an example query `\"best pizza in rome\"`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best pizza in Rome?\n",
      "Visited Rome and had one of the best pizzas of my life\n",
      "Since pizza is an American food, I'm willing to bet the best pizza is in America.\n"
     ]
    }
   ],
   "source": [
    "from praw.models import Comment\n",
    "\n",
    "# search across all subreddits for pizza recommendations\n",
    "results = reddit.subreddit(\"all\").search(\"best pizza in rome\")\n",
    "recs = []\n",
    "for submission in results:\n",
    "    title = submission.title\n",
    "    description = submission.selftext\n",
    "    # we only get comments with 20 or more upvotes\n",
    "    comments = []\n",
    "    for comment in submission.comments.list():\n",
    "        if isinstance(comment, Comment) and comment.ups >= 20:\n",
    "            author = comment.author.name if comment.author else \"unknown\"\n",
    "            comments.append(f\"{author} (upvotes: {comment.ups}): {comment.body}\")\n",
    "    # and of these, we only need 3\n",
    "    comments = comments[:3]\n",
    "    # if there are enough comments (ie 3), we add the recommendation to our list\n",
    "    if len(comments) == 3:\n",
    "        print(title)\n",
    "        recs.append(Rec(title=title, description=description, comments=comments))\n",
    "    if len(recs) == 3:\n",
    "        # stop after getting 3 recommendations\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Best pizza in Rome?\n",
      "Description: I was a little disappointed after my first experience tasting pizza after pasta and gelato were ridiculously amazing. What do you recommend? \n",
      "Comments:\n",
      "miclee15 (upvotes: 23): American here.  I think if OP is from the USA, Rome pizza needs to be approached differently.  I’m from NY where we think that is the best pizza in the US, people from Chicago will disagree.  Set aside the preconceived notion of what great pizza should be and enjoy the variety and flavors.   I’m in Rome now.  Went to Antico Forno Roscioli and had the most amazing porcetta pizza with potatoes on top.  I still love a NYC slice but Rome pizza is incredible at some places.  Edited for spelling\n",
      "Sisyphus_Rock530 (upvotes: 29): \n",
      "\n",
      "- **Pizzeria da Remo** a Testaccio, nota per la sua base sottile e croccante, è molto popolare tra i romani. https://www.romeing.it/best-pizza-in-rome/).\n",
      "\n",
      "\n",
      "- **Emma** vicino Campo de' Fiori, famosa per la sua pizza a crosta sottile e ingredienti di alta qualità (https://www.romeing.it/best-pizza-in-rome/).\n",
      "\n",
      "\n",
      "- **50 Kalò** di Ciro Salvo a Termini, conosciuta per le sue pizze particolarmente idratate e leggere, con ingredienti freschi dalla Campania (https://www.romeing.it/best-pizza-in-rome/).\n",
      "\n",
      "\n",
      "\n",
      "- **Berberè** vicino ai Giardini della Villa Borghese, offre un ambiente accogliente e pizze artigianali con ingredienti freschi (https://www.romeing.it/best-pizza-in-rome/).\n",
      "\n",
      "\n",
      "\n",
      "- **Seu Pizza Illuminati** si distingue per l'uso creativo dei condimenti e per le sue sperimentazioni sui vegetali (https://www.dissapore.com/pizzerie/le-migliori-pizzerie-di-roma-gli-indirizzi-da-provar\n",
      "Sky-Ripper (upvotes: 25): The only answer to find the best pizza is to go to Naples\n",
      "===\n",
      "Title: Visited Rome and had one of the best pizzas of my life\n",
      "Description: \n",
      "Comments:\n",
      "BubblefartsRock (upvotes: 201): i recently travelled to italy as well. the week after i came back, im not kidding when i say i was having hardcore cravings for the pizza from there. i live in a decent sized city and we don't have anything close to the quality level we saw there. enjoy while you can!!\n",
      "PopeInnocentXIV (upvotes: 82): Is that burrata in the middle?\n",
      "hahahahaha90000 (upvotes: 155): People calling it a bread bowl probably think the crust is the texture of a New York style. \n",
      "\n",
      "That crust’s texture is closer to a soufflé than bread or “pizza crust”. It’s pillowy and light and you barely have to chew to break it down. It’s divine and there’s nothing like it.\n",
      "===\n",
      "Title: Since pizza is an American food, I'm willing to bet the best pizza is in America.\n",
      "Description: \n",
      "Comments:\n",
      "unknown (upvotes: 1916): Hahaha bunch of Americans claiming pizza is from the US and then this one dude like \"sweden has the best pizza\" stay strong kid 💪\n",
      "BoglisMobileAcc (upvotes: 301): Wild take by the dude saying he was in italy for three weeks as a vegetarian and therefore only ate pizza? What..? Theres plenty of vegetarian options in italian cuisine. I’d argue italian food is one of the most vegetarian friendly cuisines in Europe.\n",
      "Scarsocontesto (upvotes: 492): the funniest comment was the dude claiming \"italians doing the best they could with what they had\" meaning he couldn't ask for topping such as salami or whatever cause they didn't have any = total bullshit\n",
      "\n",
      "the rest is really subjective. If americans love their way of doing pizza... well good for them.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n===\\n\".join([str(rec) for rec in recs]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put all of this together into a single `tool` that our LLM will be connected to for function calling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for candidates with query: software\n",
      "[Rec(title='Ella Davis', description='Sales professional with a proven track record in exceeding targets.', comments=['Candidate Ella Davis demonstrates strong potential in their field.']), Rec(title='Carol Williams', description='Creative UX/UI designer with a knack for innovative digital solutions.', comments=['Candidate Carol Williams demonstrates strong potential in their field.']), Rec(title='Carol Williams', description='Creative UX/UI designer with a knack for innovative digital solutions.', comments=['Candidate Carol Williams demonstrates strong potential in their field.'])]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def linkedin_candidate_scrapper(query: str) -> list[Rec]:\n",
    "    \"\"\"\n",
    "    Provides access to a LinkedIn candidate scrapping tool.\n",
    "    This function simulates a candidate search by returning a list\n",
    "    of random candidate recommendations. In a real implementation, it\n",
    "    would connect to LinkedIn's API or use web scraping techniques\n",
    "    to retrieve candidate profiles.\n",
    "    \"\"\"\n",
    "    sample_names = [\n",
    "        \"Alice Johnson\", \n",
    "        \"Bob Smith\", \n",
    "        \"Carol Williams\", \n",
    "        \"David Brown\", \n",
    "        \"Ella Davis\"\n",
    "    ]\n",
    "    sample_descriptions = [\n",
    "        \"Experienced software engineer with expertise in Python and Machine Learning.\",\n",
    "        \"Results-driven project manager with a strong background in tech and operations.\",\n",
    "        \"Creative UX/UI designer with a knack for innovative digital solutions.\",\n",
    "        \"Data analyst skilled in SQL, Python, and dashboard reporting.\",\n",
    "        \"Sales professional with a proven track record in exceeding targets.\"\n",
    "    ]\n",
    "    print(f\"Searching for candidates with query: {query}\")\n",
    "    candidates = []\n",
    "    # create 3 random candidate recommendations\n",
    "    for _ in range(3):\n",
    "        idx = random.randint(0, len(sample_names) - 1)\n",
    "        candidate = Rec(\n",
    "            title=sample_names[idx],\n",
    "            description=sample_descriptions[idx],\n",
    "            comments=[f\"Candidate {sample_names[idx]} demonstrates strong potential in their field.\"]\n",
    "        )\n",
    "        candidates.append(candidate)\n",
    "    return candidates\n",
    "\n",
    "# example usage:\n",
    "out = linkedin_candidate_scrapper(query=\"software\")\n",
    "print(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Answer \"Tool\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alongside our web search tool we will have a final tool called `final_answer`. The final answer tool will be called whenever the LLM has finished pulling info from the other two tools and is ready to provide a *final answer* to the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_answer(answer: str, phone_number: str = \"\", address: str = \"\"):\n",
    "    \"\"\"Returns a natural language response to the user. There are four sections \n",
    "    to be returned to the user, those are:\n",
    "    - `answer`: the final natural language answer to the user's question, should provide as much context as possible.\n",
    "    - `phone_number`: the phone number of top recommended restaurant (if found).\n",
    "    - `address`: the address of the top recommended restaurant (if found).\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"answer\": answer,\n",
    "        \"phone_number\": phone_number,\n",
    "        \"address\": address,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using LangGraph to create an agentic graph-based flow. To construct the graph we will use:\n",
    "\n",
    "* **Agent State**: An object persisted through every step in the graph, used to provide input to nodes, and to store output from nodes to be used in later nodes or in our final output.\n",
    "* **Local LLM**: We are using a local LLM (`llama-3.1:8b`) via Ollama. For tool use we turn on _JSON mode_ to reliably output parsible JSON.\n",
    "* **Tools**: The tools our LLM can use, these allow use of the functions `search` and `final_answer`.\n",
    "* **Graph Nodes**: We wrap our logic into components that allow it to be used by LangGraph, these consume and output the *Agent State*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated, List, Union\n",
    "from langchain_core.agents import AgentAction\n",
    "from langchain_core.messages import BaseMessage\n",
    "import operator\n",
    "\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    input: str\n",
    "    chat_history: list[BaseMessage]\n",
    "    intermediate_steps: Annotated[list[tuple[AgentAction, str]], operator.add]\n",
    "    output: dict[str, Union[str, List[str]]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM and Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LLM acts as our decision maker and generator of our final output, we will later call this component the `oracle` as our *decision-maker*. For this we are using Ollama and `Llama 3.1`, once initialized we integrate it into a runnable pipeline of our Oracle. The system prompt for our `oracle` will be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"You are the oracle: a specialized recruiter agent and decision-maker.\n",
    "Given the user's hiring request, decide how to proceed using the available tools.\n",
    "\n",
    "Objective:\n",
    "- Identify and recommend the best candidate(s) for the role.\n",
    "- For each recommended candidate include: name/title, key skills, years of experience, location (if known), a concise rationale for fit, contact details if available, and suggested next steps for outreach/interview.\n",
    "\n",
    "Tool-calling rules:\n",
    "- When using a tool, output ONLY one JSON object and NOTHING else, exactly matching this pattern:\n",
    "{\n",
    "    \"name\": \"<tool_name>\",\n",
    "    \"parameters\": {\"<param_key>\": <param_value>}\n",
    "}\n",
    "- Use at most one tool per turn.\n",
    "- You may call the search tool (linkedin_candidate_scrapper) up to 3 times total.\n",
    "- After any use of the search tool, you MUST call the final_answer tool to produce the human-facing summary and recommendations.\n",
    "- If the user asks something unrelated to recruiting/hiring or requests a direct answer, call final_answer directly.\n",
    "\n",
    "Behavior & response style:\n",
    "- Prioritize concise, evidence-based recommendations derived from tool outputs.\n",
    "- If results are insufficient or ambiguous, ask a focused clarifying question (do not call a tool) before searching further.\n",
    "- Always include practical next steps (e.g., outreach template, suggested interview questions, priority ranking).\n",
    "- Do not include any explanatory or narrative text when issuing a tool call — only emit the required JSON.\n",
    "\n",
    "Follow these rules strictly to ensure consistent, parseable tool usage and clear recruiter-oriented recommendations.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alongside our system prompt, we must also pass Ollama the schema of our functions for tool calls. [Tool calling](https://ollama.com/blog/tool-support) is a relatively new feature in Ollama and is used by providing function schemas to the `tools` parameter when calling our LLM.\n",
    "\n",
    "We use `FunctionSchema` object with `to_ollama` from `semantic-router` to transform our functions into correctly formatted schemas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -qU semantic-router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'convert_python_type_to_json_type' from partially initialized module 'semantic_router.utils.function_call' (most likely due to a circular import) (c:\\YoussefENSI_backup\\Eukliadia-test\\Git Repo\\.venv\\Lib\\site-packages\\semantic_router\\utils\\function_call.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msemantic_router\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunction_call\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FunctionSchema\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# create the function calling schema for ollama\u001b[39;00m\n\u001b[32m      4\u001b[39m search_schema = FunctionSchema(search).to_ollama()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\YoussefENSI_backup\\Eukliadia-test\\Git Repo\\.venv\\Lib\\site-packages\\semantic_router\\utils\\function_call.py:6\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Any, Callable, ClassVar, Dict, List, Optional, Union\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpydantic\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseModel, ConfigDict, Field\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msemantic_router\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mllms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseLLM\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msemantic_router\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mschema\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Message, RouteChoice\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msemantic_router\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlogger\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m logger\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\YoussefENSI_backup\\Eukliadia-test\\Git Repo\\.venv\\Lib\\site-packages\\semantic_router\\__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msemantic_router\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mroute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Route\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msemantic_router\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrouters\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HybridRouter, RouterConfig, SemanticRouter\n\u001b[32m      4\u001b[39m __all__ = [\u001b[33m\"\u001b[39m\u001b[33mSemanticRouter\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mHybridRouter\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mRoute\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mRouterConfig\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\YoussefENSI_backup\\Eukliadia-test\\Git Repo\\.venv\\Lib\\site-packages\\semantic_router\\route.py:7\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Any, Callable, ClassVar, Dict, List, Optional, Union\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpydantic\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseModel, ConfigDict\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msemantic_router\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mllms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseLLM\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msemantic_router\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mschema\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Message, RouteChoice\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msemantic_router\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m function_call\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\YoussefENSI_backup\\Eukliadia-test\\Git Repo\\.venv\\Lib\\site-packages\\semantic_router\\llms\\__init__.py:5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msemantic_router\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mllms\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mllamacpp\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LlamaCppLLM\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msemantic_router\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mllms\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmistral\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MistralAILLM\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msemantic_router\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mllms\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mopenai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OpenAILLM\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msemantic_router\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mllms\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mopenrouter\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OpenRouterLLM\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msemantic_router\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mllms\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mzure\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AzureOpenAILLM\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\YoussefENSI_backup\\Eukliadia-test\\Git Repo\\.venv\\Lib\\site-packages\\semantic_router\\llms\\openai.py:17\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msemantic_router\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mschema\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Message\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msemantic_router\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdefaults\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m EncoderDefault\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msemantic_router\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunction_call\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     18\u001b[39m     convert_python_type_to_json_type,\n\u001b[32m     19\u001b[39m     get_schema,\n\u001b[32m     20\u001b[39m )\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msemantic_router\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlogger\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m logger\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mOpenAILLM\u001b[39;00m(BaseLLM):\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'convert_python_type_to_json_type' from partially initialized module 'semantic_router.utils.function_call' (most likely due to a circular import) (c:\\YoussefENSI_backup\\Eukliadia-test\\Git Repo\\.venv\\Lib\\site-packages\\semantic_router\\utils\\function_call.py)"
     ]
    }
   ],
   "source": [
    "from semantic_router.utils.function_call import FunctionSchema\n",
    "\n",
    "# create the function calling schema for ollama\n",
    "search_schema = FunctionSchema(search).to_ollama()\n",
    "# TODO deafult None value for description and fix required fields in SR\n",
    "search_schema[\"function\"][\"parameters\"][\"properties\"][\"query\"][\"description\"] = None\n",
    "search_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import typing\n",
    "from typing import get_origin, get_args\n",
    "\n",
    "def _map_python_type(py_type):\n",
    "    \"\"\"Map simple python/typing types to JSON Schema types used by Ollama.\"\"\"\n",
    "    origin = get_origin(py_type)\n",
    "    args = get_args(py_type)\n",
    "    # simple builtin types\n",
    "    if py_type is str:\n",
    "        return {\"type\": \"string\"}\n",
    "    if py_type is int:\n",
    "        return {\"type\": \"integer\"}\n",
    "    if py_type is float:\n",
    "        return {\"type\": \"number\"}\n",
    "    if py_type is bool:\n",
    "        return {\"type\": \"boolean\"}\n",
    "    # list[...] -> array with items\n",
    "    if origin in (list, typing.List):\n",
    "        item_type = args[0] if args else str\n",
    "        return {\"type\": \"array\", \"items\": _map_python_type(item_type)}\n",
    "    # dict[...] -> object (loose)\n",
    "    if origin in (dict, typing.Dict):\n",
    "        return {\"type\": \"object\"}\n",
    "    # fallback\n",
    "    return {\"type\": \"string\"}\n",
    "\n",
    "def func_to_ollama_schema(func, name=None, description=None):\n",
    "    \"\"\"\n",
    "    Build a minimal Ollama function schema from a Python function.\n",
    "    Returns a dict shaped like: {\"function\": {\"name\": ..., \"description\": ..., \"parameters\": {...}}}\n",
    "    - Sets parameter descriptions to None by default (matching your TODO).\n",
    "    - Marks parameters without defaults as required.\n",
    "    \"\"\"\n",
    "    sig = inspect.signature(func)\n",
    "    params = {}\n",
    "    required = []\n",
    "    for pname, param in sig.parameters.items():\n",
    "        if param.kind in (inspect.Parameter.VAR_POSITIONAL, inspect.Parameter.VAR_KEYWORD):\n",
    "            # skip *args/**kwargs\n",
    "            continue\n",
    "        # use annotation if present, otherwise assume string\n",
    "        ann = param.annotation if param.annotation is not inspect._empty else str\n",
    "        prop_schema = _map_python_type(ann)\n",
    "        # set description to None to match your notebook TODO\n",
    "        prop_schema[\"description\"] = None\n",
    "        params[pname] = prop_schema\n",
    "        if param.default is inspect._empty:\n",
    "            required.append(pname)\n",
    "\n",
    "    parameters = {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": params,\n",
    "    }\n",
    "    if required:\n",
    "        parameters[\"required\"] = required\n",
    "\n",
    "    return {\n",
    "        \"function\": {\n",
    "            \"name\": name or func.__name__,\n",
    "            \"description\": description if description is not None else (func.__doc__ or None),\n",
    "            \"parameters\": parameters,\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'function': {'name': 'linkedin_candidate_scrapper',\n",
       "  'description': \"\\n    Provides access to a LinkedIn candidate scrapping tool.\\n    This function simulates a candidate search by returning a list\\n    of random candidate recommendations. In a real implementation, it\\n    would connect to LinkedIn's API or use web scraping techniques\\n    to retrieve candidate profiles.\\n    \",\n",
       "  'parameters': {'type': 'object',\n",
       "   'properties': {'query': {'type': 'string', 'description': None}},\n",
       "   'required': ['query']}}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_schema = func_to_ollama_schema(linkedin_candidate_scrapper, description=None)\n",
    "search_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'function': {'name': 'final_answer',\n",
       "  'description': \"Returns a natural language response to the user. There are four sections \\n    to be returned to the user, those are:\\n    - `answer`: the final natural language answer to the user's question, should provide as much context as possible.\\n    - `phone_number`: the phone number of top recommended restaurant (if found).\\n    - `address`: the address of the top recommended restaurant (if found).\\n    \",\n",
       "  'parameters': {'type': 'object',\n",
       "   'properties': {'answer': {'type': 'string', 'description': None},\n",
       "    'phone_number': {'type': 'string', 'description': None},\n",
       "    'address': {'type': 'string', 'description': None}},\n",
       "   'required': ['answer']}}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_answer_schema = func_to_ollama_schema(final_answer, description=None)\n",
    "final_answer_schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can test our LLM!\n",
    "\n",
    "---\n",
    "\n",
    "**❗️ Make sure you have Ollama running locally and you have already downloaded the model with `ollama pull llama3.1:8b`!**\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-29 02:23:22 - httpx - INFO - _client.py:1025 - _send_single_request() - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "def get_system_tools_prompt(system_prompt: str, tools: list[dict]):\n",
    "    tools_str = \"\\n\".join([str(tool) for tool in tools])\n",
    "    return (\n",
    "        f\"{system_prompt}\\n\\n\"\n",
    "        f\"You may use the following tools:\\n{tools_str}\"\n",
    "    )\n",
    "\n",
    "res = ollama.chat(\n",
    "    model=\"llama3-groq-tool-use:8b\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": get_system_tools_prompt(\n",
    "            system_prompt=system_prompt,\n",
    "            tools=[search_schema, final_answer_schema]\n",
    "        )},\n",
    "        # chat history will go here\n",
    "        {\"role\": \"user\", \"content\": \"hello there\"}\n",
    "        # scratchpad will go here\n",
    "    ],\n",
    "    format=\"json\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatResponse(model='llama3-groq-tool-use:8b', created_at='2025-09-29T01:23:22.7399572Z', done=True, done_reason='stop', total_duration=83555763200, load_duration=10593596500, prompt_eval_count=502, prompt_eval_duration=68456659000, eval_count=14, eval_duration=4497960500, message=Message(role='assistant', content='{ \"name\": \"final_answer\", \"parameters\": {} }', thinking=None, images=None, tool_name=None, tool_calls=None))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see here that the LLM is correctly deciding to use the `final_answer` tool to respond to the user. To parse this information we can use `json.loads`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'final_answer', 'parameters': {}}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "json.loads(res[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we can get it to use the reddit search tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-29 02:25:54 - httpx - INFO - _client.py:1025 - _send_single_request() - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'name': 'linkedin_candidate_scrapper',\n",
       " 'parameters': {'query': 'software engineer, python, machine learning'}}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = ollama.chat(\n",
    "    model=\"llama3-groq-tool-use:8b\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": get_system_tools_prompt(\n",
    "            system_prompt=system_prompt,\n",
    "            tools=[search_schema, final_answer_schema]\n",
    "        )},\n",
    "        # chat history will go here\n",
    "        {\"role\": \"user\", \"content\": \"hi, I'm looking for a software engnieer with experience in python and machine learning.\"}\n",
    "        # scratchpad will go here\n",
    "    ],\n",
    "    format=\"json\",\n",
    ")\n",
    "# parse the output\n",
    "json.loads(res[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, this looks perfect!\n",
    "\n",
    "To keep things a little more organized we can use another pydantic schema to organize the output from our LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentAction(tool_name='linkedin_candidate_scrapper', tool_input={'query': 'software engineer, python, machine learning'}, tool_output=None)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class AgentAction(BaseModel):\n",
    "    tool_name: str\n",
    "    tool_input: dict\n",
    "    tool_output: str | None = None\n",
    "\n",
    "    @classmethod\n",
    "    def from_ollama(cls, ollama_response: dict):\n",
    "        try:\n",
    "            # parse the output\n",
    "            output = json.loads(ollama_response[\"message\"][\"content\"])\n",
    "            return cls(\n",
    "                tool_name=output[\"name\"],\n",
    "                tool_input=output[\"parameters\"],\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing ollama response:\\n{ollama_response}\\n\")\n",
    "            raise e\n",
    "\n",
    "    def __str__(self):\n",
    "        text = f\"Tool: {self.tool_name}\\nInput: {self.tool_input}\"\n",
    "        if self.tool_output is not None:\n",
    "            text += f\"\\nOutput: {self.tool_output}\"\n",
    "        return text\n",
    "\n",
    "\n",
    "action = AgentAction.from_ollama(res)\n",
    "action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks great! Now we just need to wrap this with the ability to contain chat history and the agent scratchpad — before adding everything into our graph.\n",
    "\n",
    "For our agent actions, we will be converting them into fake back-and-forth messages between the assistant and user. For example:\n",
    "\n",
    "```python\n",
    "AgentAction(\n",
    "    tool_name=\"xyz\",\n",
    "    tool_input={\"query\": \"something cool\"},\n",
    "    tool_output=\"A fascinating tidbit of information\"\n",
    ")\n",
    "```\n",
    "\n",
    "Would become:\n",
    "\n",
    "```json\n",
    "[\n",
    "    {\"role\": \"assistant\", \"content\": \"{'name': 'xyz', 'parameters': {'query': 'something cool'}\"},\n",
    "    {\"role\": \"user\", \"content\": \"A fascinating tidbit of information\"}\n",
    "]\n",
    "```\n",
    "\n",
    "We will make this happen with an `action_to_message` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def action_to_message(action: AgentAction):\n",
    "    # create assistant \"input\" message\n",
    "    assistant_content = json.dumps({\"name\": action.tool_name, \"parameters\": action.tool_input})\n",
    "    assistant_message = {\"role\": \"assistant\", \"content\": assistant_content}\n",
    "    # create user \"response\" message\n",
    "    user_message = {\"role\": \"user\", \"content\": action.tool_output}\n",
    "    return [assistant_message, user_message]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'assistant',\n",
       "  'content': '{\"name\": \"xyz\", \"parameters\": {\"query\": \"something cool\"}}'},\n",
       " {'role': 'user', 'content': 'A fascinating tidbit of information'}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_action = AgentAction(\n",
    "    tool_name=\"xyz\",\n",
    "    tool_input={\"query\": \"something cool\"},\n",
    "    tool_output=\"A fascinating tidbit of information\"\n",
    ")\n",
    "action_to_message(test_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_scratchpad(intermediate_steps: list[AgentAction]):\n",
    "    # filter for actions that have a tool_output\n",
    "    intermediate_steps = [action for action in intermediate_steps if action.tool_output is not None]\n",
    "    # format the intermediate steps into a \"assistant\" input and \"user\" response list\n",
    "    scratch_pad_messages = []\n",
    "    for action in intermediate_steps:\n",
    "        scratch_pad_messages.extend(action_to_message(action))\n",
    "    return scratch_pad_messages\n",
    "\n",
    "def call_llm(user_input: str, chat_history: list[dict], intermediate_steps: list[AgentAction]) -> AgentAction:\n",
    "    # format the intermediate steps into a scratchpad\n",
    "    scratchpad = create_scratchpad(intermediate_steps)\n",
    "    # if the scratchpad is not empty, we add a small reminder message to the agent\n",
    "    if scratchpad:\n",
    "        scratchpad += [{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": (\n",
    "                f\"Please continue, as a reminder my query was '{user_input}'. \"\n",
    "                \"Only answer to the original query, and nothing else — but use the \"\n",
    "                \"information I provided to you to do so. Provide as much \"\n",
    "                \"information as possible in the `answer` field of the \"\n",
    "                \"final_answer tool and remember to leave the contact details \"\n",
    "                \"of a promising looking candidate.\"\n",
    "            )\n",
    "        }]\n",
    "        # we determine the list of tools available to the agent based on whether\n",
    "        # or not we have already used the search tool\n",
    "        tools_used = [action.tool_name for action in intermediate_steps]\n",
    "        tools = []\n",
    "        if \"search\" in tools_used:\n",
    "            # we do this because the LLM has a tendency to go off the rails\n",
    "            # and keep searching for the same thing\n",
    "            tools = [final_answer_schema]\n",
    "            scratchpad[-1][\"content\"] = \" You must now use the final_answer tool.\"\n",
    "        else:\n",
    "            # this shouldn't happen, but we include it just in case\n",
    "            tools = [search_schema, final_answer_schema]\n",
    "    else:\n",
    "        # this would indiciate we are on the first run, in which case we\n",
    "        # allow all tools to be used\n",
    "        tools = [search_schema, final_answer_schema]\n",
    "    # construct our list of messages\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": get_system_tools_prompt(\n",
    "            system_prompt=system_prompt,\n",
    "            tools=tools\n",
    "        )},\n",
    "        *chat_history,\n",
    "        {\"role\": \"user\", \"content\": user_input},\n",
    "        *scratchpad,\n",
    "    ]\n",
    "    res = ollama.chat(\n",
    "        model=\"llama3-groq-tool-use:8b\",\n",
    "        messages=messages,\n",
    "        format=\"json\",\n",
    "    )\n",
    "    return AgentAction.from_ollama(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try `call_llm` *with* chat history:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-29 02:29:18 - httpx - INFO - _client.py:1025 - _send_single_request() - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AgentAction(tool_name='linkedin_candidate_scrapper', tool_input={'query': 'backend Python developer with machine learning experience'}, tool_output=None)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's fake some chat history and test\n",
    "out = call_llm(\n",
    "    chat_history=[\n",
    "        {\"role\": \"user\", \"content\": \"hi im looking for a software engineer\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"okey great, can you tell me more about the role?\"},\n",
    "        {\"role\": \"user\", \"content\": \"the role is for a backend python developer with experience in machine learning\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Okay, I will start looking for candidates.\"},\n",
    "    ],\n",
    "    user_input=\"okey perfect, please go ahead\",\n",
    "    intermediate_steps=[]\n",
    ")\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We intentionally didn't include where we are in our current `user_input`, but instead included it in the `chat_history` to confirm our agent is able to consider the chat history when building our web search tool call. It succeeded! Now we can move on to constructing our graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have our search query, we can pass it onto our `search` tool to get some results, let's try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for candidates with query: backend Python developer with machine learning experience\n",
      "[Rec(title='Ella Davis', description='Sales professional with a proven track record in exceeding targets.', comments=['Candidate Ella Davis demonstrates strong potential in their field.']), Rec(title='Ella Davis', description='Sales professional with a proven track record in exceeding targets.', comments=['Candidate Ella Davis demonstrates strong potential in their field.']), Rec(title='Alice Johnson', description='Experienced software engineer with expertise in Python and Machine Learning.', comments=['Candidate Alice Johnson demonstrates strong potential in their field.'])]\n"
     ]
    }
   ],
   "source": [
    "results = linkedin_candidate_scrapper(**out.tool_input)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have our results! Each of these is pretty high level, there is not much detail as they only represent search page result descriptions. So now, we must decide which links look most promising — we can do that by passing these results onwards to another LLM that decides which result we should get more information from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have defined the different logical components of our graph, but we need to execute them in a langgraph-friendly manner — for that they must consume our `AgentState` and return modifications to that state. We will do this for all of our components via three functions:\n",
    "\n",
    "* `run_oracle` will handle running our oracle LLM.\n",
    "* `router` will handle the *routing* between our oracle and tools.\n",
    "* `run_tool` will handle running our tool functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_oracle(state: TypedDict):\n",
    "    print(\"run_oracle\")\n",
    "    chat_history = state[\"chat_history\"]\n",
    "    out = call_llm(\n",
    "        user_input=state[\"input\"],\n",
    "        chat_history=chat_history,\n",
    "        intermediate_steps=state[\"intermediate_steps\"]\n",
    "    )\n",
    "    return {\n",
    "        \"intermediate_steps\": [out]\n",
    "    }\n",
    "\n",
    "def router(state: TypedDict):\n",
    "    print(\"router\")\n",
    "    # return the tool name to use\n",
    "    if isinstance(state[\"intermediate_steps\"], list):\n",
    "        return state[\"intermediate_steps\"][-1].tool_name\n",
    "    else:\n",
    "        # if we output bad format go to final answer\n",
    "        print(\"Router invalid format\")\n",
    "        return \"final_answer\"\n",
    "\n",
    "# we use this to map tool names to tool functions\n",
    "tool_str_to_func = {\n",
    "    \"linkedin_candidate_scrapper\": linkedin_candidate_scrapper,\n",
    "    \"final_answer\": final_answer\n",
    "}\n",
    "\n",
    "def run_tool(state: TypedDict):\n",
    "    # use this as helper function so we repeat less code\n",
    "    tool_name = state[\"intermediate_steps\"][-1].tool_name\n",
    "    tool_args = state[\"intermediate_steps\"][-1].tool_input\n",
    "    print(f\"run_tool | {tool_name}.invoke(input={tool_args})\")\n",
    "    # run tool\n",
    "    out = tool_str_to_func[tool_name](**tool_args)\n",
    "    action_out = AgentAction(\n",
    "        tool_name=tool_name,\n",
    "        tool_input=tool_args,\n",
    "        tool_output=str(out),\n",
    "    )\n",
    "    if tool_name == \"final_answer\":\n",
    "        return {\"output\": out}\n",
    "    else:\n",
    "        return {\"intermediate_steps\": [action_out]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We construct our graph using `add_nodes`, `add_edge`, and `add_conditional_edges`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "graph = StateGraph(AgentState)\n",
    "\n",
    "graph.add_node(\"oracle\", run_oracle)\n",
    "graph.add_node(\"linkedin_candidate_scrapper\", run_tool)\n",
    "graph.add_node(\"final_answer\", run_tool)\n",
    "\n",
    "graph.set_entry_point(\"oracle\")  # insert query here\n",
    "\n",
    "graph.add_conditional_edges(  # - - - >\n",
    "    source=\"oracle\",  # where in graph to start\n",
    "    path=router,  # function to determine which node is called\n",
    ")\n",
    "\n",
    "# create edges from each tool back to the oracle\n",
    "for tool_obj in [search_schema, final_answer_schema]:\n",
    "    tool_name = tool_obj[\"function\"][\"name\"]\n",
    "    if tool_name != \"final_answer\":\n",
    "        graph.add_edge(tool_name, \"oracle\")  # ————————>\n",
    "\n",
    "# if anything goes to final answer, it must then move to END\n",
    "graph.add_edge(\"final_answer\", END)\n",
    "\n",
    "runnable = graph.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view the graph we can generate a mermaid graph like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Starting Chromium download.\n",
      "2025-09-29 02:30:59 - pyppeteer.chromium_downloader - INFO - chromium_downloader.py:75 - download_zip() - Starting Chromium download.\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Chromium downloadable not found at https://storage.googleapis.com/chromium-browser-snapshots/Win_x64/1181205/chrome-win.zip: Received <?xml version='1.0' encoding='UTF-8'?><Error><Code>NoSuchKey</Code><Message>The specified key does not exist.</Message><Details>No such object: chromium-browser-snapshots/Win_x64/1181205/chrome-win.zip</Details></Error>.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrunnables\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgraph\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CurveStyle, MermaidDrawMethod, NodeStyles\n\u001b[32m      5\u001b[39m nest_asyncio.apply()  \u001b[38;5;66;03m# Required for Jupyter Notebook to run async functions\u001b[39;00m\n\u001b[32m      7\u001b[39m display(\n\u001b[32m      8\u001b[39m     Image(\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m         \u001b[43mrunnable\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdraw_mermaid_png\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcurve_style\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCurveStyle\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLINEAR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnode_colors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mNodeStyles\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfirst\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m#ffdfba\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlast\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m#baffc9\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m#fad7de\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m            \u001b[49m\u001b[43mwrap_label_n_words\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m9\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m            \u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdraw_method\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMermaidDrawMethod\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPYPPETEER\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m            \u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mwhite\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m     )\n\u001b[32m     19\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\YoussefENSI_backup\\Eukliadia-test\\Git Repo\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\graph.py:607\u001b[39m, in \u001b[36mGraph.draw_mermaid_png\u001b[39m\u001b[34m(self, curve_style, node_colors, wrap_label_n_words, output_file_path, draw_method, background_color, padding)\u001b[39m\n\u001b[32m    600\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrunnables\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgraph_mermaid\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m draw_mermaid_png\n\u001b[32m    602\u001b[39m mermaid_syntax = \u001b[38;5;28mself\u001b[39m.draw_mermaid(\n\u001b[32m    603\u001b[39m     curve_style=curve_style,\n\u001b[32m    604\u001b[39m     node_colors=node_colors,\n\u001b[32m    605\u001b[39m     wrap_label_n_words=wrap_label_n_words,\n\u001b[32m    606\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m607\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw_mermaid_png\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    608\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmermaid_syntax\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmermaid_syntax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    609\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    610\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdraw_method\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdraw_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    611\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    612\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    613\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\YoussefENSI_backup\\Eukliadia-test\\Git Repo\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\graph_mermaid.py:170\u001b[39m, in \u001b[36mdraw_mermaid_png\u001b[39m\u001b[34m(mermaid_syntax, output_file_path, draw_method, background_color, padding)\u001b[39m\n\u001b[32m    167\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m draw_method == MermaidDrawMethod.PYPPETEER:\n\u001b[32m    168\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01masyncio\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m     img_bytes = \u001b[43masyncio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_render_mermaid_using_pyppeteer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmermaid_syntax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m draw_method == MermaidDrawMethod.API:\n\u001b[32m    176\u001b[39m     img_bytes = _render_mermaid_using_api(\n\u001b[32m    177\u001b[39m         mermaid_syntax, output_file_path, background_color\n\u001b[32m    178\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\YoussefENSI_backup\\Eukliadia-test\\Git Repo\\.venv\\Lib\\site-packages\\nest_asyncio.py:30\u001b[39m, in \u001b[36m_patch_asyncio.<locals>.run\u001b[39m\u001b[34m(main, debug)\u001b[39m\n\u001b[32m     28\u001b[39m task = asyncio.ensure_future(main)\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task.done():\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\YoussefENSI_backup\\Eukliadia-test\\Git Repo\\.venv\\Lib\\site-packages\\nest_asyncio.py:98\u001b[39m, in \u001b[36m_patch_loop.<locals>.run_until_complete\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f.done():\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m     97\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mEvent loop stopped before Future completed.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Python311\\Lib\\asyncio\\futures.py:203\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    201\u001b[39m \u001b[38;5;28mself\u001b[39m.__log_traceback = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    202\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception.with_traceback(\u001b[38;5;28mself\u001b[39m._exception_tb)\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Python311\\Lib\\asyncio\\tasks.py:267\u001b[39m, in \u001b[36mTask.__step\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    263\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    264\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    265\u001b[39m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[32m    266\u001b[39m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m267\u001b[39m         result = \u001b[43mcoro\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    268\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    269\u001b[39m         result = coro.throw(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\YoussefENSI_backup\\Eukliadia-test\\Git Repo\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\graph_mermaid.py:204\u001b[39m, in \u001b[36m_render_mermaid_using_pyppeteer\u001b[39m\u001b[34m(mermaid_syntax, output_file_path, background_color, padding, device_scale_factor)\u001b[39m\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    200\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m    201\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mInstall Pyppeteer to use the Pyppeteer method: `pip install pyppeteer`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    202\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m204\u001b[39m browser = \u001b[38;5;28;01mawait\u001b[39;00m launch()\n\u001b[32m    205\u001b[39m page = \u001b[38;5;28;01mawait\u001b[39;00m browser.newPage()\n\u001b[32m    207\u001b[39m \u001b[38;5;66;03m# Setup Mermaid JS\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\YoussefENSI_backup\\Eukliadia-test\\Git Repo\\.venv\\Lib\\site-packages\\pyppeteer\\launcher.py:307\u001b[39m, in \u001b[36mlaunch\u001b[39m\u001b[34m(options, **kwargs)\u001b[39m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlaunch\u001b[39m(options: \u001b[38;5;28mdict\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m, **kwargs: Any) -> Browser:\n\u001b[32m    240\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Start chrome process and return :class:`~pyppeteer.browser.Browser`.\u001b[39;00m\n\u001b[32m    241\u001b[39m \u001b[33;03m    This function is a shortcut to :meth:`Launcher(options, **kwargs).launch`.\u001b[39;00m\n\u001b[32m    242\u001b[39m \u001b[33;03m    Available options are:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    305\u001b[39m \u001b[33;03m        option with extreme caution.\u001b[39;00m\n\u001b[32m    306\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m307\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[43mLauncher\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m.launch()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\YoussefENSI_backup\\Eukliadia-test\\Git Repo\\.venv\\Lib\\site-packages\\pyppeteer\\launcher.py:120\u001b[39m, in \u001b[36mLauncher.__init__\u001b[39m\u001b[34m(self, options, **kwargs)\u001b[39m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.chromeExecutable:\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m check_chromium():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[43mdownload_chromium\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    121\u001b[39m     \u001b[38;5;28mself\u001b[39m.chromeExecutable = \u001b[38;5;28mstr\u001b[39m(chromium_executable())\n\u001b[32m    123\u001b[39m \u001b[38;5;28mself\u001b[39m.cmd = [\u001b[38;5;28mself\u001b[39m.chromeExecutable] + \u001b[38;5;28mself\u001b[39m.chromeArguments\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\YoussefENSI_backup\\Eukliadia-test\\Git Repo\\.venv\\Lib\\site-packages\\pyppeteer\\chromium_downloader.py:138\u001b[39m, in \u001b[36mdownload_chromium\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdownload_chromium\u001b[39m() -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    137\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Download and extract chromium.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m     extract_zip(\u001b[43mdownload_zip\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_url\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m, DOWNLOADS_FOLDER / REVISION)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\YoussefENSI_backup\\Eukliadia-test\\Git Repo\\.venv\\Lib\\site-packages\\pyppeteer\\chromium_downloader.py:82\u001b[39m, in \u001b[36mdownload_zip\u001b[39m\u001b[34m(url)\u001b[39m\n\u001b[32m     80\u001b[39m r = http.request(\u001b[33m'\u001b[39m\u001b[33mGET\u001b[39m\u001b[33m'\u001b[39m, url, preload_content=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m r.status >= \u001b[32m400\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mChromium downloadable not found at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[33m'\u001b[39m \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mReceived \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mr.data.decode()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m     84\u001b[39m \u001b[38;5;66;03m# 10 * 1024\u001b[39;00m\n\u001b[32m     85\u001b[39m _data = BytesIO()\n",
      "\u001b[31mOSError\u001b[39m: Chromium downloadable not found at https://storage.googleapis.com/chromium-browser-snapshots/Win_x64/1181205/chrome-win.zip: Received <?xml version='1.0' encoding='UTF-8'?><Error><Code>NoSuchKey</Code><Message>The specified key does not exist.</Message><Details>No such object: chromium-browser-snapshots/Win_x64/1181205/chrome-win.zip</Details></Error>.\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "from IPython.display import Image, display\n",
    "from langchain_core.runnables.graph import CurveStyle, MermaidDrawMethod, NodeStyles\n",
    "\n",
    "nest_asyncio.apply()  # Required for Jupyter Notebook to run async functions\n",
    "\n",
    "display(\n",
    "    Image(\n",
    "        runnable.get_graph().draw_mermaid_png(\n",
    "            curve_style=CurveStyle.LINEAR,\n",
    "            node_colors=NodeStyles(first=\"#ffdfba\", last=\"#baffc9\", default=\"#fad7de\"),\n",
    "            wrap_label_n_words=9,\n",
    "            output_file_path=None,\n",
    "            draw_method=MermaidDrawMethod.PYPPETEER,\n",
    "            background_color=\"white\",\n",
    "            padding=10,\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our agent has now been constructed so we can test it. First, let's check for the best pizza in Rome:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_oracle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-29 02:31:21 - httpx - INFO - _client.py:1025 - _send_single_request() - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "router\n",
      "run_tool | linkedin_candidate_scrapper.invoke(input={'query': 'software engineer'})\n",
      "Searching for candidates with query: software engineer\n",
      "run_oracle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-29 02:32:08 - httpx - INFO - _client.py:1025 - _send_single_request() - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "router\n",
      "run_tool | final_answer.invoke(input={'answer': 'Based on your search, I found three strong candidates: Carol Williams, Ella Davis, and David Brown. Each has demonstrated significant potential in their field. It might be worth exploring how they can contribute to your project.'})\n"
     ]
    }
   ],
   "source": [
    "out = runnable.invoke({\n",
    "    \"input\": \"hi im looking for a software engineer\",\n",
    "    \"chat_history\": [],\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extract the answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': 'Based on your question, I would recommend trying Seu Pizza Illuminati in Rome for an amazing pizza experience. They are known for their creative use of condiments and experiments with vegetables. If you prefer a Neapolitan-style pizza, Pizzeria da Remo or Emma might be the perfect choice for you. Both places have received great reviews from locals and visitors alike.',\n",
       " 'phone_number': '',\n",
       " 'address': ''}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[\"output\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are recommended [Seu Pizza Illuminati](https://maps.app.goo.gl/RMSdTUpH8D3oQETUA), a seemingly notorious pizzeria known for their less traditional and more experimental Neapolitan pizzas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
